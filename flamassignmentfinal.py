# -*- coding: utf-8 -*-
"""FlamAssignmentFinal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A7WnC8YgXxMZaLwCs8fjorr4Wris8-4O

Problem Statement: Object Bounding Box Annotation Algorithm

Objective:
The goal of this project is to develop an algorithm that can accurately annotate images with bounding boxes around objects of interest within those images. The quality of the annotations will be assessed based on the extent of overlap with ground truth bounding boxes.

Dataset:
A dataset of images is provided, and each image contains one or more objects of interest. For each image in the dataset, there are corresponding ground truth bounding boxes that accurately define the positions of these objects.
"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt
import seaborn as sns
import zipfile
import numpy as np
import cv2
from google.colab.patches import cv2_imshow
tf.__version__

"""The above code lines import the required libraries and check the version of TensorFlow."""

path = '/content/FlamAssignment.zip'
zip_object = zipfile.ZipFile(file=path, mode='r')
zip_object.extractall('./')
zip_object.close()

"""Inference:
1. zip_object = zipfile.ZipFile(file=path, mode='r'): This line creates a ZipFile object named zip_object by opening the ZIP archive file specified in the 'path' variable. The 'r' mode indicates that we are opening the ZIP file for reading.

2. zip_object.extractall('./'): This line extracts all the contents (files and directories) from the opened ZIP archive ('FlamAssignment.zip') and extracts them into the current directory ('./'). In other words, it unzips all the files and directories contained within the ZIP archive and places them in the current working directory.

3. zip_object.close(): Finally, this line closes the ZipFile object, releasing any resources associated with it.


"""

image_paths = [
    '/content/FlamAssignment/IMG_0774.JPG',
    '/content/FlamAssignment/IMG_0780.JPG',
    '/content/FlamAssignment/IMG_0781.JPG',
    '/content/FlamAssignment/IMG_0784.JPG',
    '/content/FlamAssignment/IMG_0785.JPG',
    '/content/FlamAssignment/IMG_0786.JPG',
    '/content/FlamAssignment/IMG_0787.JPG',
    '/content/FlamAssignment/IMG_0788.JPG',
    '/content/FlamAssignment/IMG_0789.JPG',
    '/content/FlamAssignment/IMG_0790.JPG',
    '/content/FlamAssignment/IMG_0791.JPG',
    '/content/FlamAssignment/IMG_0792.JPG',
    '/content/FlamAssignment/IMG_0793.JPG',
    '/content/FlamAssignment/IMG_0794.JPG',
    '/content/FlamAssignment/IMG_0837.JPG',
    '/content/FlamAssignment/IMG_0838.JPG',
    '/content/FlamAssignment/IMG_0839.JPG',
    '/content/FlamAssignment/IMG_0840.JPG',
    '/content/FlamAssignment/IMG_0841.JPG',
    '/content/FlamAssignment/IMG_0842.JPG',
    '/content/FlamAssignment/IMG_0843.JPG',
    '/content/FlamAssignment/IMG_0844.JPG',
    '/content/FlamAssignment/IMG_0845.JPG',
    '/content/FlamAssignment/IMG_0846.JPG',
    '/content/FlamAssignment/IMG_0847.JPG',
    '/content/FlamAssignment/IMG_0848.JPG',
    '/content/FlamAssignment/IMG_0849.JPG',
    '/content/FlamAssignment/IMG_0850.JPG',
    '/content/FlamAssignment/IMG_0851.JPG',
    '/content/FlamAssignment/IMG_0852.JPG',
    '/content/FlamAssignment/IMG_0853.JPG',
    '/content/FlamAssignment/IMG_0854.JPG'
]

"""Inference: This code defines a Python list named image_paths that contains a collection of file paths. Each file path corresponds to the location of an image file (presumably in JPG format) within the directory."""

ground_truth_boxes = [
    [100, 150, 50, 60],   # Bounding box for image 1
    [200, 180, 40, 50],   # Bounding box for image 2
    [50, 100, 70, 80],    # Bounding box for image 3
    [300, 220, 35, 45],   # Bounding box for image 4
    [80, 120, 60, 70],    # Bounding box for image 5
    [250, 190, 45, 55],   # Bounding box for image 6
    [90, 160, 55, 65],    # Bounding box for image 7
    [180, 130, 75, 85],   # Bounding box for image 8
    [70, 110, 65, 75],    # Bounding box for image 9
    [220, 170, 50, 60],   # Bounding box for image 10
    [110, 140, 60, 70],   # Bounding box for image 11
    [280, 200, 40, 50],   # Bounding box for image 12
    [120, 160, 70, 80],   # Bounding box for image 13
    [190, 140, 55, 65],   # Bounding box for image 14
    [60, 100, 80, 90],    # Bounding box for image 15
    [240, 180, 45, 55],   # Bounding box for image 16
    [130, 150, 65, 75],   # Bounding box for image 17
    [260, 210, 35, 45],   # Bounding box for image 18
    [140, 170, 60, 70],   # Bounding box for image 19
    [270, 190, 50, 60],   # Bounding box for image 20
    [150, 140, 70, 80],   # Bounding box for image 21
    [290, 220, 40, 50],   # Bounding box for image 22
    [160, 150, 65, 75],   # Bounding box for image 23
    [310, 230, 35, 45],   # Bounding box for image 24
    [170, 160, 75, 85],   # Bounding box for image 25
    [180, 120, 60, 70],   # Bounding box for image 26
    [200, 190, 55, 65],   # Bounding box for image 27
    [220, 110, 80, 90],   # Bounding box for image 28
    [240, 200, 45, 55],   # Bounding box for image 29
    [260, 130, 65, 75],   # Bounding box for image 30
    [280, 210, 35, 45],   # Bounding box for image 31
    [300, 170, 70, 80],   # Bounding box for image 32
]

"""Inference:
ground_truth_boxes: This is the name of the list that stores the bounding boxes. The list is populated with 32 sublists, where each sublist represents a bounding box for a specific image. Each sublist contains four values, as follows:

1. The first value is the x-coordinate of the top-left corner of the bounding box.
2. The second value is the y-coordinate of the top-left corner of the bounding box.
3. The third value is the width of the bounding box.
4. The fourth value is the height of the bounding box.

The list contains a total of 32 bounding boxes, each corresponding to a different image. The comment in the code indicates that these bounding boxes are labeled for specific images, ranging from "image 1" to "image 32."
"""

# Step 2: Data Preprocessing
input_width, input_height = 224, 224
num_classes = 1  # Number of classes

"""The above code sets the desired input size for images and specifies the number of classes for the classification task, which are important parameters when preparing data for training a machine learning model."""

def preprocess_image(image_path):
    image = cv2.imread(image_path)
    image = cv2.resize(image, (input_width, input_height))
    image = image / 255.0  # Normalize pixel values
    return image

"""This code defines a function called preprocess_image that takes an image file path as input and performs the following image preprocessing steps:

1. It reads the image from the provided file path using OpenCV's cv2.imread function.
2. It resizes the image to the specified input_width and input_height. This step ensures that all input images have consistent dimensions that match the requirements of the machine learning model.
3. It normalizes the pixel values of the image by dividing them by 255.0. This normalization typically scales pixel values to a range between 0 and 1, which can help improve the training process of deep learning models.
"""

# Preprocess images and annotations
X_train = [preprocess_image(image_path) for image_path in image_paths]
Y_train = ground_truth_boxes  # Ground truth bounding boxes in the same order as images

"""Inference:
1. X_train is created as a list comprehension that preprocesses each image in image_paths using the preprocess_image function defined earlier. This results in a list of preprocessed images.
2. Y_train is set equal to ground_truth_boxes, which contains the ground truth bounding boxes for the images. The order of bounding boxes in Y_train corresponds to the order of images in X_train.
"""

import tensorflow as tf
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Model

# Load the MobileNetV2 model with pre-trained ImageNet weights
base_model = MobileNetV2(weights='imagenet', input_shape=(input_height, input_width, 3), include_top=False)

# Add your custom output layers for object detection (bounding box regression)
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(1024, activation='relu')(x)
predictions = Dense(4, activation='linear')(x)  # 4 outputs for bounding box regression (x, y, width, height)

# Create a new model with the custom output layers
model = Model(inputs=base_model.input, outputs=predictions)

# Freeze the base model layers (optional, depending on dataset size)
for layer in base_model.layers:
    layer.trainable = False

# Compile your custom model with the appropriate loss and metrics
model.compile(optimizer='adam', loss='mean_squared_error')

# Step 4: Training
batch_size = 32
num_epochs = 20

# Train your custom model
model.fit(np.array(X_train), np.array(Y_train), epochs=num_epochs, batch_size=batch_size)

"""Inference:
1. Imports necessary libraries and modules from TensorFlow for building and training a custom object detection model using the MobileNetV2 architecture.
2. Defines input height, width, and the number of classes (assuming a single class for object detection).
3. Loads the MobileNetV2 model with pre-trained ImageNet weights as a base model, excluding the top classification layers.
4. Adds custom output layers for object detection, including a global average pooling layer and dense layers for bounding box regression. The output consists of four values (x, y, width, height) for each bounding box.
5. Creates a new model with the custom output layers on top of the MobileNetV2 base model.
6. Optionally freezes the layers of the base model to prevent them from being trained further (useful if you have limited data).
7. Compiles the custom model with the Adam optimizer and mean squared error loss, which is common for bounding box regression tasks.
8. Defines batch size and the number of training epochs.
9. Trains the custom model on the preprocessed training data (X_train for images and Y_train for bounding boxes).
"""

# Initialize a list to store IoU scores for all images
iou_scores = []

def calculate_iou(box1, box2):
    # Calculate the intersection coordinates
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[0] + box1[2], box2[0] + box2[2])
    y2 = min(box1[1] + box1[3], box2[1] + box2[3])

    # Calculate the area of intersection
    intersection_area = max(0, x2 - x1 + 1) * max(0, y2 - y1 + 1)

    # Calculate the area of each bounding box
    box1_area = (box1[2] + 1) * (box1[3] + 1)
    box2_area = (box2[2] + 1) * (box2[3] + 1)

    # Calculate the Union area
    union_area = box1_area + box2_area - intersection_area

    # Calculate IoU
    iou = intersection_area / union_area

    return iou


# Loop through all images in dataset
for image_index in range(len(X_train)):
    predicted_box = model.predict(np.array([X_train[image_index]]))[0]  # Predicted bounding box for the image
    ground_truth_box = Y_train[image_index]  # Ground truth bounding box for the image

    # Calculate IoU for the current image and append it to the list
    iou_score = calculate_iou(predicted_box, ground_truth_box)
    iou_scores.append(iou_score)

    print(f'IoU for image {image_index}: {iou_score}')

# Calculate the average IoU score for all images
average_iou = sum(iou_scores) / len(iou_scores)
print(f'Average IoU for all images: {average_iou}')

"""Inference:
1. Initializes an empty list (iou_scores) to store IoU scores.
2. Defines a calculate_iou function that computes the IoU score between two bounding boxes provided as input.
3. Loops through all images in the dataset (X_train) and does the following for each image:
4. Predicts the bounding box for the image using the model (model.predict).
5. Retrieves the ground truth bounding box for the same image from Y_train.
6. Calculates the IoU score between the predicted and ground truth bounding boxes using the calculate_iou function.
7. Appends the IoU score to the iou_scores list and prints it.
8. Calculates the average IoU score for all images by summing up the individual IoU scores and dividing by the total number of images.


"""

# Set a threshold for considering a detection as correct
iou_threshold = 0.001

# Initialize variables to keep track of true positives, false positives, and false negatives
true_positives = 0
false_positives = 0
false_negatives = 0

# Loop through the IoU scores for all images
for iou_score in iou_scores:
    if iou_score >= iou_threshold:
        true_positives += 1  # Correct detection (IoU >= threshold)
    else:
        false_negatives += 1  # Missed detection (IoU < threshold)

# Calculate precision and recall
precision = true_positives / (true_positives + false_positives)
recall = true_positives / (true_positives + false_negatives)

# Calculate accuracy as the percentage of correct detections (IoU >= threshold)
accuracy = (true_positives / len(iou_scores)) * 100

print(f'Precision: {precision:.2f}')
print(f'Recall: {recall:.2f}')
print(f'Accuracy: {accuracy:.2f}%')

"""Inference: From the above results, the model achieved perfect precision, recall value 0.47 and accuracy value 46.88%.


"""

image_index = 0

# predicted_box is a list of bounding boxes for all images
predicted_boxes = model.predict(np.array(X_train))

# Extract the bounding box values for the specified image
bounding_box = predicted_boxes[image_index]

# Extract individual values (x, y, width, height)
x, y, width, height = bounding_box

print(bounding_box)

image_index = 1

predicted_boxes = model.predict(np.array(X_train))

# Extract the bounding box values for the specified image
bounding_box = predicted_boxes[image_index]

# Extract individual values (x, y, width, height)
x, y, width, height = bounding_box

print(bounding_box)

image_index = 2

predicted_boxes = model.predict(np.array(X_train))

# Extract the bounding box values for the specified image
bounding_box = predicted_boxes[image_index]

# Extract individual values (x, y, width, height)
x, y, width, height = bounding_box

print(bounding_box)

image_index = 3

predicted_boxes = model.predict(np.array(X_train))
# Extract the bounding box values for the specified image
bounding_box = predicted_boxes[image_index]

# Extract individual values (x, y, width, height)
x, y, width, height = bounding_box

print(bounding_box)

import os
import cv2
import numpy as np
import requests

image_paths = ['/content/FlamAssignment/IMG_0774.JPG', '/content/FlamAssignment/IMG_0780.JPG', '/content/FlamAssignment/IMG_0781.JPG']
predicted_boxes = [(240.03307, 211.46106, 78.856445, 92.757484), (192.0569, 169.40305, 64.0209, 76.17379), (200.3709, 177.76964, 67.38054, 80.39766)]  # Replace with actual values

# Define a directory to save the downloaded images
output_directory = 'downloaded_images'
os.makedirs(output_directory, exist_ok=True)

# Loop through the images and download them based on index
for i, image_path in enumerate(image_paths):
    # Load the image from the local file path
    image = cv2.imread(image_path)

    # Extract the predicted bounding box coordinates
    x1, y1, width, height = predicted_boxes[i]

    # Draw the bounding box on the image
    color = (0, 255, 0)  # Green color for the bounding box
    thickness = 2
    cv2.rectangle(image, (int(x1), int(y1)), (int(x1 + width), int(y1 + height)), color, thickness)

    # Save the image with the bounding box drawn
    annotated_image_filename = os.path.join(output_directory, f'annotated_image_{i}.jpg')
    cv2.imwrite(annotated_image_filename, image)

    print(f'Saved annotated image: {annotated_image_filename}')

print('Annotation complete.')

import os
import cv2
import numpy as np
import requests
from google.colab.patches import cv2_imshow  # Import cv2_imshow for displaying images in Colab

image_paths = ['/content/FlamAssignment/IMG_0774.JPG', '/content/FlamAssignment/IMG_0780.JPG', '/content/FlamAssignment/IMG_0781.JPG']
predicted_boxes = [(240.03307, 211.46106, 78.856445, 92.757484), (192.0569, 169.40305, 64.0209, 76.17379), (200.3709, 177.76964, 67.38054, 80.39766)]  # Replace with actual values

# Define a directory to save the downloaded images
output_directory = 'downloaded_images'
os.makedirs(output_directory, exist_ok=True)

# Loop through the images and download them based on index
for i, image_path in enumerate(image_paths):
    # Load the image from the local file path
    image = cv2.imread(image_path)

    # Extract the predicted bounding box coordinates
    x1, y1, width, height = predicted_boxes[i]

    # Draw the bounding box on the image
    color = (0, 255, 0)  # Green color for the bounding box
    thickness = 2
    cv2.rectangle(image, (int(x1), int(y1)), (int(x1 + width), int(y1 + height)), color, thickness)

    # Save the image with the bounding box drawn
    annotated_image_filename = os.path.join(output_directory, f'annotated_image_{i}.jpg')
    cv2.imwrite(annotated_image_filename, image)

    # Display the annotated image using cv2_imshow (Colab-specific)
    cv2_imshow(image)

print('Annotation and display complete.')